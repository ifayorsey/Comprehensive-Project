```{r, include=FALSE}
library(RJSONIO) 
library(RCurl) 
library(ngram)
library(tm)
library(wordcloud)
library(ggplot2)
library(glmnet)
library(text2vec)
library(data.table)
library(magrittr)
library(dplyr)
library(mosaic)
library(RTextTools)
library(e1071)
# library(caret)
library(tidyr)
library(stringr)
library(jsonlite)
library(tidytext)
library(rpart)
library(lessR)
library(knitr)
library(kableExtra)
library(randomForest)
#library(MASS) #lda 
```


```{r, include=FALSE}
setwd("~/Desktop/Statistics/Comps/Comps - Fayorsey/Comps-Fayorsey19E")
data <- read.csv("cleaned_hm.csv", stringsAsFactors = FALSE)

data$predicted_category <- as.factor(data$predicted_category)
```

```{r loading, include=FALSE, cache=TRUE}
set.seed(57)

bin_data <- data[sample(1:nrow(data), 15000), ]
my.matrix1 <- create_matrix(bin_data$cleaned_hm, language="english", removeNumbers=TRUE, stemWords=FALSE, weighting=tm::weightTfIdf)

container1 <- create_container(my.matrix1, bin_data$predicted_category, trainSize=1:8000, testSize =8001:10000,virgin = FALSE)

```

```{r svm, include=FALSE, cache=TRUE}
set.seed(6)
start_svm <- Sys.time()
svm_model <- train_model(container1, "SVM")
end_svm <- Sys.time()

svm_results <- classify_model(container1, svm_model)
ac_svm <- mean(svm_results$SVM_LABEL != bin_data[8001:10000,9])

table(svm_results$SVM_LABEL, bin_data[8001:10000,9])


end_svm - start_svm

pr_svm <- create_precisionRecallSummary(container1, svm_results)
```

```{r tree, include=FALSE, cache=TRUE}
set.seed(1)
start_tree <- Sys.time()
tree_model <- train_model(container1, "TREE")
end_tree <- Sys.time()

tree_results <- classify_model(container1, tree_model)
table(tree_results$TREE_LABEL, bin_data[8001:10000,9])

ac_tree <- (682+363+202+39)/2000
end_tree - start_tree

pr_tree <- create_precisionRecallSummary(container1, tree_results)
```

```{r glm, include=FALSE, cache=TRUE}
set.seed(2)
start_multi <- Sys.time()
multinomial_model <- train_model(container1, "GLMNET", family="multinomial")
end_multi <- Sys.time()

multi_results <- classify_model(container1, multinomial_model)
table(multi_results$GLMNET_LABEL, bin_data[8001:10000,9])
ac_multi <- mean(multi_results$GLMNET_LABEL != bin_data[8001:10000,9])
end_multi - start_multi

pr_multi <- create_precisionRecallSummary(container1, multi_results)
```

```{r bagging, include=FALSE, cache=TRUE}
# set.seed(3)
# start_bagging <- Sys.time()
# lda_model <- train_model(container1, "BAGGING")
# end_bagging <- Sys.time()
# 
# bagging_results <- classify_model(container1, lda_model)
# table(bagging_results$BAGGING_LABEL, bin_data[8001:10000,9])
# ac_bagging <- mean(bagging_results$BAGGING_LABEL != bin_data[8001:10000,9])
# 
# end_bagging - start_bagging
# pr_bag <- create_precisionRecallSummary(container1, bagging_results)
```

```{r randomforest, include=FALSE, cache=TRUE}
set.seed(4)
start_rf <- Sys.time()
rf_model <- train_model(container1, "RF", ntree=10)
end_rf <- Sys.time()

rf_results <- classify_model(container1, rf_model)
table( rf_results$FORESTS_LABEL, bin_data[8001:10000,9])
ac_rf <- mean(rf_results$FORESTS_LABEL != bin_data[8001:10000,9])
end_rf-start_rf

pr_rf <- create_precisionRecallSummary(container1, rf_results)
```

```{r lda, include=FALSE, cache=TRUE}
set.seed(5)
start_lda <- Sys.time()
lda_model <- train_model(container1, "SLDA")
end_lda <- Sys.time()

lda_results <- classify_model(container1, lda_model)
table(lda_results$SLDA_LABEL, bin_data[8001:10000,9])
ac_lda <- mean(lda_results$SLDA_LABEL != bin_data[8001:10000,9])
end_lda - start_lda

pr_lda <- create_precisionRecallSummary(container1, lda_results)
```


```{r, include=FALSE}
time_svm <- end_svm - start_svm

# time_bagging <- end_bagging - start_bagging

time_rf <- end_rf - start_rf

time_multi <- end_multi - start_multi

time_lda <- end_lda - start_lda

time_tree <- end_tree - start_tree

times <- c(time_svm, time_multi, time_tree, time_rf, time_lda) 
accuracy <- c(ac_svm, ac_multi, ac_tree, ac_rf, ac_lda)

times <- as.data.frame(as.numeric(unlist(times)))

colnames(times) <- c("runtime")


model <- c("SVM", "Multinomial", "Decision Trees", "Random Forests", "LDA")

run_table <- cbind(model, accuracy, times)

roundval <- 3
```


# Results {#results}


The goal of this analysis is to compare the ability of different learning algorithms tasked with text categorization. Responses have been transformed into document term matrices, accounting for the presence of each word in the description of happy moments. Five models are trained on a random sample of 8,000 observations, and one of five categories is predicted on a unlabeled test set of 2,000 responses. 

## Run Times

The amount of time taken to learn the 8,000 responses varied substantially across all methods. The linear SVM and decision tree were amongst the fastest trained models taking just `r round(run_table[1,3], roundval)` and `r round(run_table[3,3], roundval)` seconds respectively. The multinomial logistic was the fastest model, taking just  `r round(run_table[2,3], roundval)` seconds. 

The higher runtime for the random forest model follows convention given that it is an aggregation of several learning methods. The runtime of the LDA model initially came as a surprise, however LDA computes discriminant scores $\delta_k(x)$ by finding a linear combination of the independent variables. In a highly sparse setting, this becomes a linear combination of over 10,000 variables for each of the 6 categories. The training time of the SVM, decision tree, and multinomial models were quite impressive, given the nature of the problem when applied to larger data sets.


```{r, echo=FALSE, fig.cap = "Performance Metrics", results='asis'}
kable(run_table, format = "latex")
```

## Classification Accuracy

As stated in the introduction the goal of this analysis was to compare classification error rates between models. SVMs had the lowest misclassification rate with `r round(ac_svm, roundval)`.  This was a slight improvement on the multinomial model (`r round(ac_multi, roundval)` misclassification rate). LDA and Random Forest performed suprisingling well (`r round(ac_lda, roundval)` for LDA and `r round(ac_rf, roundval)` for rf). The decision tree produced the highest misclassification rate with `r round(ac_tree, roundval)` incorrectly categorized. Figure 3.1 shows the misclassification rates for all five algorithms tested.

```{r, echo=FALSE, fig.cap = "Bar Plot of Missclasification Rate"}
ggplot()+
  geom_bar(aes(x= reorder(run_table$model, run_table$accuracy), y=run_table$accuracy), stat="identity")+
  labs(x="Model", y="Percentage Misclassified", title="Missclasstion Rates")+
  guides(fill="none")
```

	
While accuracy is a valid measure for information retrieval, in situations where classes are imbalanced, always predicting one class may yield a high accuracy. Precision and recall can be used to focus the evaluation on the correctly label categories (true positives). Precision is defined as the proportion of items *i* placed in the category correctly out of every algorithmic declaration of *i*, while recall is the proportion of items correctly guessed that truly belong in the category (Platt, 5). Low precision-high recall systems guess a label frequently, however a significant portion of labels are incorrect. High precision-low recall systems are the opposite, predicting a class less frequently but more accurately. Ideal systems have both high precision and high recall,will return many results, with all results labeled correctly. The table below consists of precision-recall values for the category achievement for all 5 algorithms.

```{r, echo=FALSE, , fig.cap = "Precision Recall Table for Category Achievement"}
pr_table <- rbind(pr_svm[1,1:2], pr_multi[1,1:2], pr_tree[1,1:2], pr_rf[1,1:2], pr_lda[1,1:2])

rownames(pr_table) <- c("SVM", "Multinomial", "Decision Tree", "Random Forests", "LDA")

kable(pr_table, caption = "Precision-Recall Table for Achievement Category", format = "latex")

```

The most efficient classifier (SVM) had high precision and high recall for the category achievement. Although the decision tree predicted achievement frequently, only 0.49 were correctly labeled. Interestingly enough, this was also this case for the multinomial model, which had a low precision of 0.64 and high recall of 0.96. However unlike the decision tree, the overall misclassification rate for the multinomial was quite low (`r ac_multi`% labeled incorrectly). When their confusion matrices are compared, it is immediately apparent that two classes were not predicted in the decision tree model. Due to its greedy nature, the decision tree appeared to overfit on the noise in the training data. This leads to poor generalizability, and the algorithm only predicting the majority categories. On the other hand, the multinomial model accurately predicted a high proportion of the other classes. 


```{r treeconfusion, echo=FALSE, results='asis'}
table1 <- table(tree_results$TREE_LABEL, bin_data[8001:10000,9])
kable(table1[,1:7], format= "latex", caption= "Decision Tree Confusion Matrix \\label{tab:treeconfusion}") 

```

```{r multi, echo=FALSE, results='asis'}
table2 <- table(multi_results$GLMNET_LABEL, bin_data[8001:10000,9])
kable(table2[,1:6], format = "latex", caption = "Multinomial Confusion Matrix \\label{tab:multi}")

```


 



